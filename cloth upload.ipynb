{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 1: Setup and Imports ---\n\nprint(\"Importing libraries...\")\nimport os\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# Suppress TensorFlow informational messages\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.layers import GlobalMaxPooling2D\nfrom sklearn.neighbors import NearestNeighbors\nfrom numpy.linalg import norm\n\nprint(\"--> Libraries imported successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T11:15:45.388070Z","iopub.execute_input":"2025-07-19T11:15:45.388522Z","iopub.status.idle":"2025-07-19T11:16:07.195001Z","shell.execute_reply.started":"2025-07-19T11:15:45.388502Z","shell.execute_reply":"2025-07-19T11:16:07.194304Z"}},"outputs":[{"name":"stdout","text":"Importing libraries...\n","output_type":"stream"},{"name":"stderr","text":"2025-07-19 11:15:48.902104: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752923749.270120      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752923749.372478      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"--> Libraries imported successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Cell 2: Define Paths and Load Core Models ---\n\nprint(\"Defining file paths and loading models...\")\n\n# Define File Paths\nBASE_PATH = '/kaggle/input/clothestry/clothes_tryon_dataset/'\n# We no longer need the query image path, but we still need the database path\nCLOTH_DATABASE_PATH = os.path.join(BASE_PATH, 'train', 'cloth')\nOUTPUT_PATH = '/kaggle/working/'\n\n# Load TFLite Segmentation Model\ntry:\n    tflite_model_path = '/kaggle/input/deeplabv3-xception65/tflite/ade20k/2/2.tflite'\n    segmentation_interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n    segmentation_interpreter.allocate_tensors()\n    segmentation_input_details = segmentation_interpreter.get_input_details()\n    segmentation_output_details = segmentation_interpreter.get_output_details()\n    print(\"--> TFLite Segmentation model loaded successfully.\")\nexcept Exception as e:\n    print(f\"!!! FATAL ERROR loading TFLite segmentation model: {e}\")\n    raise\n\n# Load Feature Extraction Model (ResNet50)\ntry:\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    base_model.trainable = False\n    model_feature_extractor = tf.keras.Sequential([base_model, GlobalMaxPooling2D()])\n    print(\"--> Feature extraction model (ResNet50) created.\")\nexcept Exception as e:\n    print(f\"!!! FATAL ERROR creating feature extraction model: {e}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T11:16:07.195953Z","iopub.execute_input":"2025-07-19T11:16:07.196502Z","iopub.status.idle":"2025-07-19T11:16:13.581135Z","shell.execute_reply.started":"2025-07-19T11:16:07.196481Z","shell.execute_reply":"2025-07-19T11:16:13.580236Z"}},"outputs":[{"name":"stdout","text":"Defining file paths and loading models...\n--> TFLite Segmentation model loaded successfully.\n","output_type":"stream"},{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nI0000 00:00:1752923770.040931      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752923770.041665      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n--> Feature extraction model (ResNet50) created.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- Cell 3: Define Helper Functions (with Maximum Padding) ---\n\nprint(\"Defining helper functions...\")\n\n# This function uses the ResNet50 model and does not need changes.\ndef extract_features(img_pil, model):\n    \"\"\"Takes a PIL Image and extracts a normalized feature vector.\"\"\"\n    try:\n        img_resized = img_pil.resize((224, 224))\n        img_array = np.array(img_resized)\n        expand_img = np.expand_dims(img_array, axis=0)\n        pre_img = preprocess_input(expand_img)\n        result = model.predict(pre_img, verbose=0).flatten()\n        return result / norm(result)\n    except Exception as e:\n        print(f\"Error during feature extraction: {e}\")\n        return None\n\n# This function is modified to have a much larger crop area.\ndef segment_and_crop_garment(image_path, interpreter, input_details, output_details):\n    \"\"\"Detects clothing using a TFLite interpreter and returns a cropped PIL Image.\"\"\"\n    original_image_pil = Image.open(image_path).convert(\"RGB\")\n    original_image_np = np.array(original_image_pil)\n    \n    _, input_height, input_width, _ = input_details[0]['shape']\n    resized_image = cv2.resize(original_image_np, (input_width, input_height))\n    input_data = np.expand_dims(resized_image, axis=0).astype(np.float32)\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    segmentation_mask = interpreter.get_tensor(output_details[0]['index'])[0]\n    segmentation_mask = np.argmax(segmentation_mask, axis=-1)\n    \n    top_classes = [13] \n    \n    binary_mask = np.zeros_like(segmentation_mask, dtype=bool)\n    for class_id in top_classes:\n        binary_mask = binary_mask | (segmentation_mask == class_id)\n        \n    mask_resized_pil = Image.fromarray(binary_mask.astype(np.uint8) * 255)\n    mask_resized_pil = mask_resized_pil.resize(original_image_pil.size, Image.NEAREST)\n    mask_resized = np.array(mask_resized_pil).astype(bool)\n    \n    if not np.any(mask_resized): return None\n\n    where = np.where(mask_resized)\n    (ymin, ymax), (xmin, xmax) = (np.min(where[0]), np.max(where[0])), (np.min(where[1]), np.max(where[1]))\n    \n    # !!! MODIFICATION: Increased padding for a much larger crop area !!!\n    padding = 60\n    \n    cropped_image = original_image_pil.crop((\n        max(0, xmin - padding), max(0, ymin - padding),\n        min(original_image_np.shape[1], xmax + padding), min(original_image_np.shape[0], ymax + padding)\n    ))\n    return cropped_image\n\nprint(\"--> Helper functions defined with a larger crop area (padding=60).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T11:46:36.285337Z","iopub.execute_input":"2025-07-19T11:46:36.285639Z","iopub.status.idle":"2025-07-19T11:46:36.296630Z","shell.execute_reply.started":"2025-07-19T11:46:36.285614Z","shell.execute_reply":"2025-07-19T11:46:36.295991Z"}},"outputs":[{"name":"stdout","text":"Defining helper functions...\n--> Helper functions defined with a larger crop area (padding=60).\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# --- NEW Cell 4: Create and Save the Feature Database ---\n\n# Define the paths for the output files\nfeature_list_path = os.path.join(OUTPUT_PATH, 'embedding.pkl')\nfilenames_path = os.path.join(OUTPUT_PATH, 'filenames.pkl')\n\n# This 'if' statement is key: it checks if the files already exist in this session.\nif not os.path.exists(feature_list_path) or not os.path.exists(filenames_path):\n    print(\"Feature database not found in this session. Creating a new one...\")\n    print(\"(This is a one-time process per session and may take 15-30 minutes.)\")\n    \n    # Get all the file paths from your clothing database\n    cloth_filenames = [os.path.join(CLOTH_DATABASE_PATH, f) for f in sorted(os.listdir(CLOTH_DATABASE_PATH))]\n    feature_list = []\n\n    # Loop through all files and extract features, showing a progress bar\n    for filename in tqdm(cloth_filenames, desc=\"Extracting Features\"):\n        try:\n            img_pil = Image.open(filename).convert(\"RGB\")\n            features = extract_features(img_pil, model_feature_extractor)\n            if features is not None:\n                feature_list.append(features)\n        except Exception as e:\n            print(f\"Skipping file {filename} due to error: {e}\")\n    \n    # Save the completed lists to disk\n    with open(feature_list_path, 'wb') as f:\n        pickle.dump(feature_list, f)\n    with open(filenames_path, 'wb') as f:\n        pickle.dump(cloth_filenames, f)\n        \n    print(f\"\\n--> Database created and saved successfully to '{OUTPUT_PATH}'\")\nelse:\n    print(f\"--> Found existing feature database in this session. Skipping creation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T11:18:55.994235Z","iopub.execute_input":"2025-07-19T11:18:55.994573Z","iopub.status.idle":"2025-07-19T11:40:12.409289Z","shell.execute_reply.started":"2025-07-19T11:18:55.994551Z","shell.execute_reply":"2025-07-19T11:40:12.408256Z"}},"outputs":[{"name":"stdout","text":"Feature database not found in this session. Creating a new one...\n(This is a one-time process per session and may take 15-30 minutes.)\n","output_type":"stream"},{"name":"stderr","text":"Extracting Features:   0%|          | 0/11647 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1752923939.193054     100 service.cc:148] XLA service 0x7950f4002650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1752923939.194528     100 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752923939.194549     100 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752923939.919526     100 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1752923943.085244     100 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nExtracting Features: 100%|██████████| 11647/11647 [21:16<00:00,  9.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--> Database created and saved successfully to '/kaggle/working/'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- Cell 4: Load Pre-computed Database and Initialize Recommenders ---\n\nprint(\"Loading feature database for recommendation...\")\nfeature_list_path = os.path.join(OUTPUT_PATH, 'embedding.pkl')\nfilenames_path = os.path.join(OUTPUT_PATH, 'filenames.pkl')\n\ntry:\n    feature_list = np.array(pickle.load(open(feature_list_path, 'rb')))\n    filenames = pickle.load(open(filenames_path, 'rb'))\n    print(f\"--> Database with {len(feature_list)} embeddings loaded.\")\nexcept FileNotFoundError:\n    print(\"!!! ERROR: Database files (embedding.pkl, filenames.pkl) not found.\")\n    print(\"!!! Please run the feature extraction notebook/cell first to create them.\")\n    raise\nexcept Exception as e:\n    print(f\"!!! FATAL ERROR loading database files: {e}\")\n    raise\n\n# Initialize TWO Recommenders with Different Metrics for 10 neighbors\nneighbors_euclidean = NearestNeighbors(n_neighbors=10, algorithm='brute', metric='euclidean')\nneighbors_euclidean.fit(feature_list)\nprint(\"--> Euclidean Recommender is ready.\")\n\nneighbors_cosine = NearestNeighbors(n_neighbors=10, algorithm='brute', metric='cosine')\nneighbors_cosine.fit(feature_list)\nprint(\"--> Cosine Recommender is ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T11:40:53.660363Z","iopub.execute_input":"2025-07-19T11:40:53.661232Z","iopub.status.idle":"2025-07-19T11:40:53.862724Z","shell.execute_reply.started":"2025-07-19T11:40:53.661201Z","shell.execute_reply":"2025-07-19T11:40:53.862093Z"}},"outputs":[{"name":"stdout","text":"Loading feature database for recommendation...\n--> Database with 11647 embeddings loaded.\n--> Euclidean Recommender is ready.\n--> Cosine Recommender is ready.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# --- Cell 5: The Interactive Upload and Recommendation Cell ---\n\n# This is the main function that runs when you upload an image\ndef find_recommendations(uploaded_file_data):\n    # --- Pipeline Step 1: Save the uploaded file temporarily ---\n    file_name = uploaded_file_data['name']\n    file_content = uploaded_file_data['content']\n    temp_path = f\"./{file_name}\"\n    with open(temp_path, 'wb') as f:\n        f.write(file_content)\n\n    # --- Pipeline Step 2: Segment the garment from the uploaded image ---\n    cropped_garment = segment_and_crop_garment(\n        temp_path, \n        segmentation_interpreter, \n        segmentation_input_details, \n        segmentation_output_details\n    )\n    \n    # Clean up the temporary file immediately\n    os.remove(temp_path)\n\n    # --- Pipeline Step 3: Run the rest of the pipeline ---\n    if cropped_garment:\n        print(\"1. Garment detected and cropped.\")\n        query_features = extract_features(cropped_garment, model_feature_extractor)\n        \n        if query_features is not None:\n            print(\"2. Features extracted from cropped garment.\")\n            \n            # Display Query Analysis\n            fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n            axes[0].imshow(Image.open(io.BytesIO(file_content))); axes[0].set_title(\"Your Upload\"); axes[0].axis('off')\n            axes[1].imshow(cropped_garment); axes[1].set_title(\"Auto-Cropped Garment\"); axes[1].axis('off')\n            plt.suptitle(\"Query Analysis\", fontsize=16); plt.show()\n            \n            # A) Get and Display Euclidean Results\n            distances_euc, indices_euc = neighbors_euclidean.kneighbors([query_features])\n            print(\"\\n\" + \"=\"*50 + \"\\n      RESULTS USING EUCLIDEAN DISTANCE (Top 10)\\n\" + \"=\"*50)\n            print(\"--> Lower distance is better.\")\n            for i in range(10):\n                file_code = os.path.basename(filenames[indices_euc[0][i]])\n                print(f\"  Rank {i+1:02d}: Euclidean Distance = {distances_euc[0][i]:.4f}, File Code = {file_code}\")\n            \n            fig, axes = plt.subplots(2, 5, figsize=(20, 8)); axes = axes.flatten()\n            for i, idx in enumerate(indices_euc[0]):\n                axes[i].imshow(Image.open(filenames[idx]))\n                axes[i].set_title(f\"Euc Rec {i+1}\\nDist: {distances_euc[0][i]:.2f}\"); axes[i].axis('off')\n            plt.tight_layout(); plt.show()\n\n            # B) Get and Display Cosine Results\n            distances_cos, indices_cos = neighbors_cosine.kneighbors([query_features])\n            print(\"\\n\" + \"=\"*50 + \"\\n        RESULTS USING COSINE SIMILARITY (Top 10)\\n\" + \"=\"*50)\n            print(\"--> Higher similarity is better.\")\n            for i in range(10):\n                similarity = 1 - distances_cos[0][i]\n                file_code = os.path.basename(filenames[indices_cos[0][i]])\n                print(f\"  Rank {i+1:02d}: Cosine Similarity = {similarity:.4f}, File Code = {file_code}\")\n                \n            fig, axes = plt.subplots(2, 5, figsize=(20, 8)); axes = axes.flatten()\n            for i, idx in enumerate(indices_cos[0]):\n                similarity = 1 - distances_cos[0][i]\n                axes[i].imshow(Image.open(filenames[idx]))\n                axes[i].set_title(f\"Cos Rec {i+1}\\nSim: {similarity:.2f}\"); axes[i].axis('off')\n            plt.tight_layout(); plt.show()\n        else:\n            print(\"!!! FAILED: Could not extract features from the cropped garment.\")\n    else:\n        print(\"\\n!!! FAILED: Could not detect a garment in your uploaded image.\")\n        display(Image.open(io.BytesIO(file_content)))\n\n# --- Widget Setup ---\n# We need this to handle the uploaded file data\nimport io\n\nuploader = widgets.FileUpload(accept='image/*', multiple=False, description='Upload an Image')\noutput_area = widgets.Output()\n\ndef on_upload_change(change):\n    with output_area:\n        clear_output(wait=True)\n        # The new syntax for ipywidgets gets the data from the first item in the value tuple\n        if uploader.value:\n            find_recommendations(uploader.value[0])\n\nuploader.observe(on_upload_change, names='value')\n\nprint(\"Please upload an image to start the recommendation process.\")\ndisplay(uploader, output_area)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T11:46:42.304704Z","iopub.execute_input":"2025-07-19T11:46:42.304999Z","iopub.status.idle":"2025-07-19T11:46:42.324888Z","shell.execute_reply.started":"2025-07-19T11:46:42.304977Z","shell.execute_reply":"2025-07-19T11:46:42.324188Z"}},"outputs":[{"name":"stdout","text":"Please upload an image to start the recommendation process.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='image/*', description='Upload an Image')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09ed97f9cb77424ba4549f85b633e6fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b870fa7bbdcc4efebb1b3ac015b89832"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}